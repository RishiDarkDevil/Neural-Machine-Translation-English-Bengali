{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural-Machine-Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOb3ZbN6dzhLMa2D8InH78H"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting Google Drive"
      ],
      "metadata": {
        "id": "0dqNGaFLKKAJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVenoImvJZs9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/IASNLP\""
      ],
      "metadata": {
        "id": "iNpRDc1-KMj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "PfWtwH6jKV-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install --upgrade -q jax\n",
        "!pip install --upgrade -q jaxlib\n",
        "!pip install --upgrade -q trax"
      ],
      "metadata": {
        "id": "THQjQh3IKSYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sentencepiece as spm\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import trax\n",
        "from trax.data import inputs\n",
        "from trax.fastmath import numpy as fnp\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "from trax.models import Transformer\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from functools import reduce\n",
        "from collections import Counter\n",
        "import os"
      ],
      "metadata": {
        "id": "BtRDGlTFKYkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "metadata": {
        "id": "-mlIIHTcxDjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Data Generator"
      ],
      "metadata": {
        "id": "dGSyrTxsdEwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the `train_data` and `train_dev_data` along with the SentencePiece BPE models for English and Bengali."
      ],
      "metadata": {
        "id": "vs3sicrGV62k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_dev_data = pd.read_csv(\"train_data.csv\")[['src', 'tgt']], pd.read_csv(\"train_dev.csv\")[['src', 'tgt']]\n",
        "sp_en_bpe, sp_ben_bpe = spm.SentencePieceProcessor(), spm.SentencePieceProcessor()\n",
        "sp_en_bpe.load('eng_bpe.model'); sp_ben_bpe.load('ben_bpe.model');"
      ],
      "metadata": {
        "id": "dwRB4a6-TMo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions are taken from the data preprocessing part to get tokenized representation of sentences as well as detokenized sentences."
      ],
      "metadata": {
        "id": "RecOkU19WJ5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence, sp_model):\n",
        "    # We add the EOS token at the end of each encoded sentence\n",
        "    inputs = sp_model.encode_as_ids(sentence) + [sp_model.eos_id()]\n",
        "    return np.reshape(np.array(inputs), [1, -1])\n",
        "def detokenize(tokenized, sp_model):\n",
        "    integers = np.squeeze(tokenized).tolist()\n",
        "    return sp_model.DecodeIdsWithCheck(integers[:integers.index(sp_model.eos_id())])\n",
        "def data_generator(batch_size, src, tgt, maxlen=60, shuffle=False, verbose=False):\n",
        "    num_lines = len(src)\n",
        "    lines_index = [*range(num_lines)]\n",
        "    if shuffle:\n",
        "        np.random.shuffle(lines_index)\n",
        "    index = 0\n",
        "    while True:\n",
        "        buffer_src = list()\n",
        "        buffer_tgt = list() \n",
        "        max_len = 0 \n",
        "        for i in range(batch_size):\n",
        "            if index >= num_lines:\n",
        "                index = 0\n",
        "                if shuffle:\n",
        "                    np.random.shuffle(lines_index)\n",
        "            buffer_src.append(src[lines_index[index]])\n",
        "            buffer_tgt.append(tgt[lines_index[index]])\n",
        "            index += 1\n",
        "        batch_src = pad_sequences(buffer_src, maxlen = maxlen, padding='post', truncating='post')\n",
        "        batch_tgt = pad_sequences(buffer_tgt, maxlen = maxlen, padding='post', truncating='post')\n",
        "        if verbose: print(\"index=\", index)\n",
        "        yield((batch_src, batch_tgt))"
      ],
      "metadata": {
        "id": "BG712QbdjiDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_train_data_enc = [tokenize(train_data['src'].iloc[i], sp_en_bpe) for i in range(train_data.shape[0])]\n",
        "tgt_train_data_enc = [tokenize(train_data['tgt'].iloc[i], sp_ben_bpe) for i in range(train_data.shape[0])]\n",
        "src_train_dev_data_enc = [tokenize(train_dev_data['src'].iloc[i], sp_en_bpe) for i in range(train_dev_data.shape[0])]\n",
        "tgt_train_dev_data_enc = [tokenize(train_dev_data['tgt'].iloc[i], sp_ben_bpe) for i in range(train_dev_data.shape[0])]"
      ],
      "metadata": {
        "id": "sKWfGPHSkIiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generators for `train_data` and `train_dev_data`."
      ],
      "metadata": {
        "id": "4_IkM3sSZI0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_gen = data_generator(128, src_train_data_enc, tgt_train_data_enc)\n",
        "train_dev_data_gen = data_generator(32, src_train_dev_data_enc, tgt_train_dev_data_enc)"
      ],
      "metadata": {
        "id": "l1j2-VXOo-Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "daRvdVJNlYuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Model-1"
      ],
      "metadata": {
        "id": "oAjtw5l5oQVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Basic Transformer"
      ],
      "metadata": {
        "id": "GjwwhLS5jojv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to train our first model. We will start by training a basic transformer model from scratch with `embedding dimension = 256`, `dense layer units = 512`, `number of heads = 4`, `number of encoder layers = number of decoder layers = 3` and `maximum number of tokens = 60`.\\\n",
        "The model takes two inputs i.e. the source and corresponding target sentences as a tuple and results in two outputs i.e. "
      ],
      "metadata": {
        "id": "WgAcGVLjlbx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(input_vocab_size=16000, output_vocab_size=16000, d_model=256, d_ff=512, dropout = 0.1, n_heads=4, n_encoder_layers=3, n_decoder_layers=3, max_len=60, mode='train')\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEdPj8fBkUQ9",
        "outputId": "7cef2ab7-0355-4901-9ad6-5a778be321b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Serial_in2_out2[\n",
              "  Select[0,1,1]_in2_out3\n",
              "  Branch_out2[\n",
              "    []\n",
              "    Serial[\n",
              "      PaddingMask(0)\n",
              "    ]\n",
              "  ]\n",
              "  Serial_in2_out2[\n",
              "    Embedding_16000_256\n",
              "    Dropout\n",
              "    PositionalEncoding\n",
              "    Serial_in2_out2[\n",
              "      Branch_in2_out3[\n",
              "        None\n",
              "        Serial_in2_out2[\n",
              "          LayerNorm\n",
              "          Serial_in2_out2[\n",
              "            _in2_out2\n",
              "            Serial_in2_out2[\n",
              "              Select[0,0,0]_out3\n",
              "              Serial_in4_out2[\n",
              "                _in4_out4\n",
              "                Serial_in4_out2[\n",
              "                  Parallel_in3_out3[\n",
              "                    Dense_256\n",
              "                    Dense_256\n",
              "                    Dense_256\n",
              "                  ]\n",
              "                  PureAttention_in4_out2\n",
              "                  Dense_256\n",
              "                ]\n",
              "                _in2_out2\n",
              "              ]\n",
              "            ]\n",
              "            _in2_out2\n",
              "          ]\n",
              "          Dropout\n",
              "        ]\n",
              "      ]\n",
              "      Add_in2\n",
              "    ]\n",
              "    Serial[\n",
              "      Branch_out2[\n",
              "        None\n",
              "        Serial[\n",
              "          LayerNorm\n",
              "          Dense_512\n",
              "          Serial[\n",
              "            Relu\n",
              "          ]\n",
              "          Dropout\n",
              "          Dense_256\n",
              "          Dropout\n",
              "        ]\n",
              "      ]\n",
              "      Add_in2\n",
              "    ]\n",
              "    Serial_in2_out2[\n",
              "      Branch_in2_out3[\n",
              "        None\n",
              "        Serial_in2_out2[\n",
              "          LayerNorm\n",
              "          Serial_in2_out2[\n",
              "            _in2_out2\n",
              "            Serial_in2_out2[\n",
              "              Select[0,0,0]_out3\n",
              "              Serial_in4_out2[\n",
              "                _in4_out4\n",
              "                Serial_in4_out2[\n",
              "                  Parallel_in3_out3[\n",
              "                    Dense_256\n",
              "                    Dense_256\n",
              "                    Dense_256\n",
              "                  ]\n",
              "                  PureAttention_in4_out2\n",
              "                  Dense_256\n",
              "                ]\n",
              "                _in2_out2\n",
              "              ]\n",
              "            ]\n",
              "            _in2_out2\n",
              "          ]\n",
              "          Dropout\n",
              "        ]\n",
              "      ]\n",
              "      Add_in2\n",
              "    ]\n",
              "    Serial[\n",
              "      Branch_out2[\n",
              "        None\n",
              "        Serial[\n",
              "          LayerNorm\n",
              "          Dense_512\n",
              "          Serial[\n",
              "            Relu\n",
              "          ]\n",
              "          Dropout\n",
              "          Dense_256\n",
              "          Dropout\n",
              "        ]\n",
              "      ]\n",
              "      Add_in2\n",
              "    ]\n",
              "    Serial_in2_out2[\n",
              "      Branch_in2_out3[\n",
              "        None\n",
              "        Serial_in2_out2[\n",
              "          LayerNorm\n",
              "          Serial_in2_out2[\n",
              "            _in2_out2\n",
              "            Serial_in2_out2[\n",
              "              Select[0,0,0]_out3\n",
              "              Serial_in4_out2[\n",
              "                _in4_out4\n",
              "                Serial_in4_out2[\n",
              "                  Parallel_in3_out3[\n",
              "                    Dense_256\n",
              "                    Dense_256\n",
              "                    Dense_256\n",
              "                  ]\n",
              "                  PureAttention_in4_out2\n",
              "                  Dense_256\n",
              "                ]\n",
              "                _in2_out2\n",
              "              ]\n",
              "            ]\n",
              "            _in2_out2\n",
              "          ]\n",
              "          Dropout\n",
              "        ]\n",
              "      ]\n",
              "      Add_in2\n",
              "    ]\n",
              "    Serial[\n",
              "      Branch_out2[\n",
              "        None\n",
              "        Serial[\n",
              "          LayerNorm\n",
              "          Dense_512\n",
              "          Serial[\n",
              "            Relu\n",
              "          ]\n",
              "          Dropout\n",
              "          Dense_256\n",
              "          Dropout\n",
              "        ]\n",
              "      ]\n",
              "      Add_in2\n",
              "    ]\n",
              "    LayerNorm\n",
              "  ]\n",
              "  Select[2,1,0]_in3_out3\n",
              "  Serial[\n",
              "    ShiftRight(1)\n",
              "  ]\n",
              "  Embedding_16000_256\n",
              "  Dropout\n",
              "  PositionalEncoding\n",
              "  Branch_in2_out2[\n",
              "    []\n",
              "    EncoderDecoderMask_in2\n",
              "  ]\n",
              "  Serial[\n",
              "    Branch_out2[\n",
              "      None\n",
              "      Serial[\n",
              "        LayerNorm\n",
              "        Serial[\n",
              "          Serial[\n",
              "            Serial[\n",
              "              Branch_out3[\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "              ]\n",
              "              DotProductCausalAttention_in3\n",
              "              Serial[\n",
              "                MergeHeads\n",
              "              ]\n",
              "              Dense_256\n",
              "            ]\n",
              "          ]\n",
              "        ]\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial_in3_out3[\n",
              "    Branch_in3_out4[\n",
              "      None\n",
              "      Serial_in3_out3[\n",
              "        LayerNorm\n",
              "        Select[0,2,2,1,2]_in3_out5\n",
              "        Serial_in4_out2[\n",
              "          _in4_out4\n",
              "          Serial_in4_out2[\n",
              "            Parallel_in3_out3[\n",
              "              Dense_256\n",
              "              Dense_256\n",
              "              Dense_256\n",
              "            ]\n",
              "            PureAttention_in4_out2\n",
              "            Dense_256\n",
              "          ]\n",
              "          _in2_out2\n",
              "        ]\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial[\n",
              "    Branch_out2[\n",
              "      None\n",
              "      Serial[\n",
              "        LayerNorm\n",
              "        Dense_512\n",
              "        Serial[\n",
              "          Relu\n",
              "        ]\n",
              "        Dropout\n",
              "        Dense_256\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial[\n",
              "    Branch_out2[\n",
              "      None\n",
              "      Serial[\n",
              "        LayerNorm\n",
              "        Serial[\n",
              "          Serial[\n",
              "            Serial[\n",
              "              Branch_out3[\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "              ]\n",
              "              DotProductCausalAttention_in3\n",
              "              Serial[\n",
              "                MergeHeads\n",
              "              ]\n",
              "              Dense_256\n",
              "            ]\n",
              "          ]\n",
              "        ]\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial_in3_out3[\n",
              "    Branch_in3_out4[\n",
              "      None\n",
              "      Serial_in3_out3[\n",
              "        LayerNorm\n",
              "        Select[0,2,2,1,2]_in3_out5\n",
              "        Serial_in4_out2[\n",
              "          _in4_out4\n",
              "          Serial_in4_out2[\n",
              "            Parallel_in3_out3[\n",
              "              Dense_256\n",
              "              Dense_256\n",
              "              Dense_256\n",
              "            ]\n",
              "            PureAttention_in4_out2\n",
              "            Dense_256\n",
              "          ]\n",
              "          _in2_out2\n",
              "        ]\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial[\n",
              "    Branch_out2[\n",
              "      None\n",
              "      Serial[\n",
              "        LayerNorm\n",
              "        Dense_512\n",
              "        Serial[\n",
              "          Relu\n",
              "        ]\n",
              "        Dropout\n",
              "        Dense_256\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial[\n",
              "    Branch_out2[\n",
              "      None\n",
              "      Serial[\n",
              "        LayerNorm\n",
              "        Serial[\n",
              "          Serial[\n",
              "            Serial[\n",
              "              Branch_out3[\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "                [Dense_256, Serial[\n",
              "                  SplitIntoHeads\n",
              "                ]]\n",
              "              ]\n",
              "              DotProductCausalAttention_in3\n",
              "              Serial[\n",
              "                MergeHeads\n",
              "              ]\n",
              "              Dense_256\n",
              "            ]\n",
              "          ]\n",
              "        ]\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial_in3_out3[\n",
              "    Branch_in3_out4[\n",
              "      None\n",
              "      Serial_in3_out3[\n",
              "        LayerNorm\n",
              "        Select[0,2,2,1,2]_in3_out5\n",
              "        Serial_in4_out2[\n",
              "          _in4_out4\n",
              "          Serial_in4_out2[\n",
              "            Parallel_in3_out3[\n",
              "              Dense_256\n",
              "              Dense_256\n",
              "              Dense_256\n",
              "            ]\n",
              "            PureAttention_in4_out2\n",
              "            Dense_256\n",
              "          ]\n",
              "          _in2_out2\n",
              "        ]\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial[\n",
              "    Branch_out2[\n",
              "      None\n",
              "      Serial[\n",
              "        LayerNorm\n",
              "        Dense_512\n",
              "        Serial[\n",
              "          Relu\n",
              "        ]\n",
              "        Dropout\n",
              "        Dense_256\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  LayerNorm\n",
              "  Select[0]_in3\n",
              "  Dense_16000\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Training"
      ],
      "metadata": {
        "id": "OuDkwt7ToNGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting our data generator to be fed to the training."
      ],
      "metadata": {
        "id": "ayJ8KiEkcrEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(43)\n",
        "train_generator = trax.data.inputs.add_loss_weights(train_data_gen, id_to_mask= sp_en_bpe.pad_id())\n",
        "train_dev_generator = trax.data.inputs.add_loss_weights(train_dev_data_gen, id_to_mask= sp_en_bpe.pad_id())"
      ],
      "metadata": {
        "id": "p3_51sXIrjo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up `train_task` on which our model will be trained. We use `loss function = CrossEntropyLossWithLogSotmax`, `optimizer = Adam with learning rate 0.01`, `learning rate schedule = warm up and square root decay`."
      ],
      "metadata": {
        "id": "56uMNo9IczRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_task = training.TrainTask(labeled_data= train_generator, \n",
        "                                loss_layer= tl.CrossEntropyLossWithLogSoftmax(),\n",
        "                                optimizer= trax.optimizers.Adam(0.01),\n",
        "                                lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
        "                                n_steps_per_checkpoint= 100,\n",
        "                                n_steps_per_permanent_checkpoint= 1000)"
      ],
      "metadata": {
        "id": "jN-P4002meBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up `eval_task` for evaluating our model performance. We monitor two metrics on our evaluation dataset i.e. `train_dev_data` which are `CrossEntropyLossWithLogSoftmax` and `WeightedCategoryAccuracy`."
      ],
      "metadata": {
        "id": "JoeK7WZGdd_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_task = training.EvalTask(labeled_data=train_dev_generator,\n",
        "                              metrics=[tl.CrossEntropyLossWithLogSoftmax(), tl.WeightedCategoryAccuracy()])"
      ],
      "metadata": {
        "id": "s2aLEm9-qo7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the `training_loop` for training our model."
      ],
      "metadata": {
        "id": "VnlDe0kZeUGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = './Model'\n",
        "training_loop = training.Loop(model,\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)"
      ],
      "metadata": {
        "id": "taWsxxV1shCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training_loop.run(5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT4I3ymcs5np",
        "outputId": "0275fd78-f9e8-4268-8762-ae4471a43820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step    100: Ran 99 train steps in 118.46 secs\n",
            "Step    100: train CrossEntropyLossWithLogSoftmax |  8.31935406\n",
            "Step    100: eval  CrossEntropyLossWithLogSoftmax |  7.54077768\n",
            "Step    100: eval        WeightedCategoryAccuracy |  0.10845987\n",
            "\n",
            "Step    200: Ran 100 train steps in 42.03 secs\n",
            "Step    200: train CrossEntropyLossWithLogSoftmax |  7.46800280\n",
            "Step    200: eval  CrossEntropyLossWithLogSoftmax |  6.85576916\n",
            "Step    200: eval        WeightedCategoryAccuracy |  0.16158536\n",
            "\n",
            "Step    300: Ran 100 train steps in 44.55 secs\n",
            "Step    300: train CrossEntropyLossWithLogSoftmax |  7.26776886\n",
            "Step    300: eval  CrossEntropyLossWithLogSoftmax |  7.23670197\n",
            "Step    300: eval        WeightedCategoryAccuracy |  0.10821643\n",
            "\n",
            "Step    400: Ran 100 train steps in 44.16 secs\n",
            "Step    400: train CrossEntropyLossWithLogSoftmax |  7.04190826\n",
            "Step    400: eval  CrossEntropyLossWithLogSoftmax |  6.91076279\n",
            "Step    400: eval        WeightedCategoryAccuracy |  0.15454544\n",
            "\n",
            "Step    500: Ran 100 train steps in 49.38 secs\n",
            "Step    500: train CrossEntropyLossWithLogSoftmax |  6.85026884\n",
            "Step    500: eval  CrossEntropyLossWithLogSoftmax |  6.76692724\n",
            "Step    500: eval        WeightedCategoryAccuracy |  0.13800904\n",
            "\n",
            "Step    600: Ran 100 train steps in 46.18 secs\n",
            "Step    600: train CrossEntropyLossWithLogSoftmax |  6.75522995\n",
            "Step    600: eval  CrossEntropyLossWithLogSoftmax |  6.28391933\n",
            "Step    600: eval        WeightedCategoryAccuracy |  0.15130024\n",
            "\n",
            "Step    700: Ran 100 train steps in 47.08 secs\n",
            "Step    700: train CrossEntropyLossWithLogSoftmax |  6.70708036\n",
            "Step    700: eval  CrossEntropyLossWithLogSoftmax |  6.73426151\n",
            "Step    700: eval        WeightedCategoryAccuracy |  0.13752456\n",
            "\n",
            "Step    800: Ran 100 train steps in 46.83 secs\n",
            "Step    800: train CrossEntropyLossWithLogSoftmax |  6.63746643\n",
            "Step    800: eval  CrossEntropyLossWithLogSoftmax |  6.61461496\n",
            "Step    800: eval        WeightedCategoryAccuracy |  0.13518885\n",
            "\n",
            "Step    900: Ran 100 train steps in 46.57 secs\n",
            "Step    900: train CrossEntropyLossWithLogSoftmax |  6.63621521\n",
            "Step    900: eval  CrossEntropyLossWithLogSoftmax |  6.64254951\n",
            "Step    900: eval        WeightedCategoryAccuracy |  0.12655601\n",
            "\n",
            "Step   1000: Ran 100 train steps in 62.38 secs\n",
            "Step   1000: train CrossEntropyLossWithLogSoftmax |  6.66851377\n",
            "Step   1000: eval  CrossEntropyLossWithLogSoftmax |  6.65040731\n",
            "Step   1000: eval        WeightedCategoryAccuracy |  0.13346612\n",
            "\n",
            "Step   1100: Ran 100 train steps in 46.61 secs\n",
            "Step   1100: train CrossEntropyLossWithLogSoftmax |  6.66310358\n",
            "Step   1100: eval  CrossEntropyLossWithLogSoftmax |  6.45758438\n",
            "Step   1100: eval        WeightedCategoryAccuracy |  0.14285715\n",
            "\n",
            "Step   1200: Ran 100 train steps in 46.42 secs\n",
            "Step   1200: train CrossEntropyLossWithLogSoftmax |  6.70834827\n",
            "Step   1200: eval  CrossEntropyLossWithLogSoftmax |  6.76600027\n",
            "Step   1200: eval        WeightedCategoryAccuracy |  0.14093959\n",
            "\n",
            "Step   1300: Ran 100 train steps in 45.66 secs\n",
            "Step   1300: train CrossEntropyLossWithLogSoftmax |  6.76428270\n",
            "Step   1300: eval  CrossEntropyLossWithLogSoftmax |  6.98154163\n",
            "Step   1300: eval        WeightedCategoryAccuracy |  0.12213740\n",
            "\n",
            "Step   1400: Ran 100 train steps in 46.33 secs\n",
            "Step   1400: train CrossEntropyLossWithLogSoftmax |  6.71470928\n",
            "Step   1400: eval  CrossEntropyLossWithLogSoftmax |  6.83534861\n",
            "Step   1400: eval        WeightedCategoryAccuracy |  0.13528335\n",
            "\n",
            "Step   1500: Ran 100 train steps in 46.92 secs\n",
            "Step   1500: train CrossEntropyLossWithLogSoftmax |  6.73487949\n",
            "Step   1500: eval  CrossEntropyLossWithLogSoftmax |  6.86508608\n",
            "Step   1500: eval        WeightedCategoryAccuracy |  0.11827958\n",
            "\n",
            "Step   1600: Ran 100 train steps in 46.53 secs\n",
            "Step   1600: train CrossEntropyLossWithLogSoftmax |  6.64171076\n",
            "Step   1600: eval  CrossEntropyLossWithLogSoftmax |  6.39392614\n",
            "Step   1600: eval        WeightedCategoryAccuracy |  0.15170941\n",
            "\n",
            "Step   1700: Ran 100 train steps in 49.84 secs\n",
            "Step   1700: train CrossEntropyLossWithLogSoftmax |  6.58204651\n",
            "Step   1700: eval  CrossEntropyLossWithLogSoftmax |  6.74156046\n",
            "Step   1700: eval        WeightedCategoryAccuracy |  0.12248996\n",
            "\n",
            "Step   1800: Ran 100 train steps in 46.51 secs\n",
            "Step   1800: train CrossEntropyLossWithLogSoftmax |  6.55308628\n",
            "Step   1800: eval  CrossEntropyLossWithLogSoftmax |  6.64200640\n",
            "Step   1800: eval        WeightedCategoryAccuracy |  0.15135135\n",
            "\n",
            "Step   1900: Ran 100 train steps in 46.91 secs\n",
            "Step   1900: train CrossEntropyLossWithLogSoftmax |  6.49758816\n",
            "Step   1900: eval  CrossEntropyLossWithLogSoftmax |  6.38030910\n",
            "Step   1900: eval        WeightedCategoryAccuracy |  0.14141415\n",
            "\n",
            "Step   2000: Ran 100 train steps in 63.00 secs\n",
            "Step   2000: train CrossEntropyLossWithLogSoftmax |  6.45555830\n",
            "Step   2000: eval  CrossEntropyLossWithLogSoftmax |  6.38528633\n",
            "Step   2000: eval        WeightedCategoryAccuracy |  0.15524194\n",
            "\n",
            "Step   2100: Ran 100 train steps in 44.66 secs\n",
            "Step   2100: train CrossEntropyLossWithLogSoftmax |  6.53307676\n",
            "Step   2100: eval  CrossEntropyLossWithLogSoftmax |  6.55288649\n",
            "Step   2100: eval        WeightedCategoryAccuracy |  0.14512922\n",
            "\n",
            "Step   2200: Ran 100 train steps in 46.89 secs\n",
            "Step   2200: train CrossEntropyLossWithLogSoftmax |  6.50069141\n",
            "Step   2200: eval  CrossEntropyLossWithLogSoftmax |  6.27098989\n",
            "Step   2200: eval        WeightedCategoryAccuracy |  0.16350710\n",
            "\n",
            "Step   2300: Ran 100 train steps in 46.78 secs\n",
            "Step   2300: train CrossEntropyLossWithLogSoftmax |  6.44813967\n",
            "Step   2300: eval  CrossEntropyLossWithLogSoftmax |  6.61070967\n",
            "Step   2300: eval        WeightedCategoryAccuracy |  0.12883435\n",
            "\n",
            "Step   2400: Ran 100 train steps in 46.89 secs\n",
            "Step   2400: train CrossEntropyLossWithLogSoftmax |  6.42998171\n",
            "Step   2400: eval  CrossEntropyLossWithLogSoftmax |  6.46818495\n",
            "Step   2400: eval        WeightedCategoryAccuracy |  0.13817330\n",
            "\n",
            "Step   2500: Ran 100 train steps in 48.94 secs\n",
            "Step   2500: train CrossEntropyLossWithLogSoftmax |  6.42006683\n",
            "Step   2500: eval  CrossEntropyLossWithLogSoftmax |  6.52923250\n",
            "Step   2500: eval        WeightedCategoryAccuracy |  0.15400845\n",
            "\n",
            "Step   2600: Ran 100 train steps in 46.93 secs\n",
            "Step   2600: train CrossEntropyLossWithLogSoftmax |  6.36983633\n",
            "Step   2600: eval  CrossEntropyLossWithLogSoftmax |  6.81838322\n",
            "Step   2600: eval        WeightedCategoryAccuracy |  0.11057693\n",
            "\n",
            "Step   2700: Ran 100 train steps in 47.39 secs\n",
            "Step   2700: train CrossEntropyLossWithLogSoftmax |  6.33394527\n",
            "Step   2700: eval  CrossEntropyLossWithLogSoftmax |  6.12622643\n",
            "Step   2700: eval        WeightedCategoryAccuracy |  0.15644822\n",
            "\n",
            "Step   2800: Ran 100 train steps in 47.81 secs\n",
            "Step   2800: train CrossEntropyLossWithLogSoftmax |  6.29130888\n",
            "Step   2800: eval  CrossEntropyLossWithLogSoftmax |  6.54176331\n",
            "Step   2800: eval        WeightedCategoryAccuracy |  0.14699793\n",
            "\n",
            "Step   2900: Ran 100 train steps in 46.33 secs\n",
            "Step   2900: train CrossEntropyLossWithLogSoftmax |  6.30167913\n",
            "Step   2900: eval  CrossEntropyLossWithLogSoftmax |  6.70105886\n",
            "Step   2900: eval        WeightedCategoryAccuracy |  0.10855263\n",
            "\n",
            "Step   3000: Ran 100 train steps in 69.34 secs\n",
            "Step   3000: train CrossEntropyLossWithLogSoftmax |  6.31136656\n",
            "Step   3000: eval  CrossEntropyLossWithLogSoftmax |  6.35142326\n",
            "Step   3000: eval        WeightedCategoryAccuracy |  0.14259598\n",
            "\n",
            "Step   3100: Ran 100 train steps in 46.22 secs\n",
            "Step   3100: train CrossEntropyLossWithLogSoftmax |  6.28035975\n",
            "Step   3100: eval  CrossEntropyLossWithLogSoftmax |  6.57754230\n",
            "Step   3100: eval        WeightedCategoryAccuracy |  0.11320755\n",
            "\n",
            "Step   3200: Ran 100 train steps in 47.06 secs\n",
            "Step   3200: train CrossEntropyLossWithLogSoftmax |  6.25712061\n",
            "Step   3200: eval  CrossEntropyLossWithLogSoftmax |  6.53685665\n",
            "Step   3200: eval        WeightedCategoryAccuracy |  0.13257575\n",
            "\n",
            "Step   3300: Ran 100 train steps in 45.09 secs\n",
            "Step   3300: train CrossEntropyLossWithLogSoftmax |  6.23385191\n",
            "Step   3300: eval  CrossEntropyLossWithLogSoftmax |  6.51651335\n",
            "Step   3300: eval        WeightedCategoryAccuracy |  0.13043478\n",
            "\n",
            "Step   3400: Ran 100 train steps in 45.54 secs\n",
            "Step   3400: train CrossEntropyLossWithLogSoftmax |  6.18685293\n",
            "Step   3400: eval  CrossEntropyLossWithLogSoftmax |  6.25954151\n",
            "Step   3400: eval        WeightedCategoryAccuracy |  0.16063347\n",
            "\n",
            "Step   3500: Ran 100 train steps in 44.51 secs\n",
            "Step   3500: train CrossEntropyLossWithLogSoftmax |  6.18585873\n",
            "Step   3500: eval  CrossEntropyLossWithLogSoftmax |  6.28540945\n",
            "Step   3500: eval        WeightedCategoryAccuracy |  0.14223194\n",
            "\n",
            "Step   3600: Ran 100 train steps in 44.28 secs\n",
            "Step   3600: train CrossEntropyLossWithLogSoftmax |  6.16408300\n",
            "Step   3600: eval  CrossEntropyLossWithLogSoftmax |  6.42665911\n",
            "Step   3600: eval        WeightedCategoryAccuracy |  0.16035634\n",
            "\n",
            "Step   3700: Ran 100 train steps in 46.94 secs\n",
            "Step   3700: train CrossEntropyLossWithLogSoftmax |  6.17006111\n",
            "Step   3700: eval  CrossEntropyLossWithLogSoftmax |  6.39340258\n",
            "Step   3700: eval        WeightedCategoryAccuracy |  0.13831776\n",
            "\n",
            "Step   3800: Ran 100 train steps in 45.01 secs\n",
            "Step   3800: train CrossEntropyLossWithLogSoftmax |  6.11562252\n",
            "Step   3800: eval  CrossEntropyLossWithLogSoftmax |  6.19241095\n",
            "Step   3800: eval        WeightedCategoryAccuracy |  0.16374269\n",
            "\n",
            "Step   3900: Ran 100 train steps in 44.87 secs\n",
            "Step   3900: train CrossEntropyLossWithLogSoftmax |  6.11037350\n",
            "Step   3900: eval  CrossEntropyLossWithLogSoftmax |  6.12615490\n",
            "Step   3900: eval        WeightedCategoryAccuracy |  0.15835142\n",
            "\n",
            "Step   4000: Ran 100 train steps in 64.55 secs\n",
            "Step   4000: train CrossEntropyLossWithLogSoftmax |  6.14899349\n",
            "Step   4000: eval  CrossEntropyLossWithLogSoftmax |  6.60676098\n",
            "Step   4000: eval        WeightedCategoryAccuracy |  0.11657558\n",
            "\n",
            "Step   4100: Ran 100 train steps in 46.11 secs\n",
            "Step   4100: train CrossEntropyLossWithLogSoftmax |  6.12641811\n",
            "Step   4100: eval  CrossEntropyLossWithLogSoftmax |  6.01506948\n",
            "Step   4100: eval        WeightedCategoryAccuracy |  0.15827337\n",
            "\n",
            "Step   4200: Ran 100 train steps in 54.92 secs\n",
            "Step   4200: train CrossEntropyLossWithLogSoftmax |  6.07385492\n",
            "Step   4200: eval  CrossEntropyLossWithLogSoftmax |  6.57605505\n",
            "Step   4200: eval        WeightedCategoryAccuracy |  0.13543598\n",
            "\n",
            "Step   4300: Ran 100 train steps in 44.88 secs\n",
            "Step   4300: train CrossEntropyLossWithLogSoftmax |  6.06408453\n",
            "Step   4300: eval  CrossEntropyLossWithLogSoftmax |  6.08047485\n",
            "Step   4300: eval        WeightedCategoryAccuracy |  0.16919740\n",
            "\n",
            "Step   4400: Ran 100 train steps in 46.37 secs\n",
            "Step   4400: train CrossEntropyLossWithLogSoftmax |  6.05565834\n",
            "Step   4400: eval  CrossEntropyLossWithLogSoftmax |  6.24004269\n",
            "Step   4400: eval        WeightedCategoryAccuracy |  0.13271606\n",
            "\n",
            "Step   4500: Ran 100 train steps in 50.10 secs\n",
            "Step   4500: train CrossEntropyLossWithLogSoftmax |  6.07676077\n",
            "Step   4500: eval  CrossEntropyLossWithLogSoftmax |  6.00420380\n",
            "Step   4500: eval        WeightedCategoryAccuracy |  0.13705586\n",
            "\n",
            "Step   4600: Ran 100 train steps in 44.86 secs\n",
            "Step   4600: train CrossEntropyLossWithLogSoftmax |  6.06068659\n",
            "Step   4600: eval  CrossEntropyLossWithLogSoftmax |  6.35389328\n",
            "Step   4600: eval        WeightedCategoryAccuracy |  0.13996626\n",
            "\n",
            "Step   4700: Ran 100 train steps in 48.78 secs\n",
            "Step   4700: train CrossEntropyLossWithLogSoftmax |  6.03268051\n",
            "Step   4700: eval  CrossEntropyLossWithLogSoftmax |  6.43542576\n",
            "Step   4700: eval        WeightedCategoryAccuracy |  0.17021276\n",
            "\n",
            "Step   4800: Ran 100 train steps in 44.73 secs\n",
            "Step   4800: train CrossEntropyLossWithLogSoftmax |  6.01520443\n",
            "Step   4800: eval  CrossEntropyLossWithLogSoftmax |  6.13410568\n",
            "Step   4800: eval        WeightedCategoryAccuracy |  0.13953489\n",
            "\n",
            "Step   4900: Ran 100 train steps in 45.97 secs\n",
            "Step   4900: train CrossEntropyLossWithLogSoftmax |  6.00201273\n",
            "Step   4900: eval  CrossEntropyLossWithLogSoftmax |  6.15552282\n",
            "Step   4900: eval        WeightedCategoryAccuracy |  0.16743121\n",
            "\n",
            "Step   5000: Ran 100 train steps in 64.15 secs\n",
            "Step   5000: train CrossEntropyLossWithLogSoftmax |  6.00123644\n",
            "Step   5000: eval  CrossEntropyLossWithLogSoftmax |  5.98688698\n",
            "Step   5000: eval        WeightedCategoryAccuracy |  0.13052632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If Model directory not already present with the trained model weights, uncomment and run the previous line and this line\n",
        "# training_loop.load_checkpoint(directory=output_dir, filename=\"model.pkl.gz\")"
      ],
      "metadata": {
        "id": "fE8XQtup-src"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.init_from_file(\"./Model/model.pkl.gz\")"
      ],
      "metadata": {
        "id": "SwwdNibtiHsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Testing"
      ],
      "metadata": {
        "id": "TbYDUMNUjdyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Greedy Search"
      ],
      "metadata": {
        "id": "v6X0CZo4kJOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At every step of prediction, we feed in the input sequence and the predicted translated part upto that point. Based on this, the model predicts the next token id given the previous were the ones predicted till that point. We have `temperature=0.0`, so it will not sample the next token instead it will choose the one with maximum probability. "
      ],
      "metadata": {
        "id": "Y8HbKJVok0WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_symbol(model, input_tokens, cur_output_tokens, temperature):\n",
        "    token_length = len(cur_output_tokens)\n",
        "    padded_length = 60\n",
        "    padded = cur_output_tokens + [0] * (padded_length - token_length) \n",
        "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
        "    output, _ = model((input_tokens, padded_with_batch))   \n",
        "    log_probs = output[0, token_length, :]\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
        "    return symbol, float(log_probs[symbol])"
      ],
      "metadata": {
        "id": "Eww1p5UpuXQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling_decode(input_sentence, model = None, temperature=0.0):\n",
        "    input_tokens = tokenize(input_sentence, sp_en_bpe)\n",
        "    cur_output_tokens = []\n",
        "    cur_output = 0  \n",
        "    EOS = 1\n",
        "    while cur_output != EOS: \n",
        "        cur_output, log_prob = next_symbol(model, input_tokens, cur_output_tokens, temperature)\n",
        "        cur_output_tokens.append(cur_output) \n",
        "    sentence = detokenize(cur_output_tokens, sp_ben_bpe)\n",
        "    return cur_output_tokens, log_prob, sentence"
      ],
      "metadata": {
        "id": "qph6-_sh8Wp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode_test(sentence, model=None):  \n",
        "    _,_, translated_sentence = sampling_decode(sentence, model)   \n",
        "    return translated_sentence"
      ],
      "metadata": {
        "id": "wRRrPFnu8fk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I love you.'\n",
        "translated_sentence = greedy_decode_test(sentence, model)\n",
        "print(\"English: \", sentence)\n",
        "print(\"Bengali: \", translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaxploMw87fW",
        "outputId": "e8b1fff2-6ae0-443d-d3f6-f08b4dfc9a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:  I love you.\n",
            "Bengali:  আমি আমার কথা বলার।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Minimum Bayes-Risk Decoding"
      ],
      "metadata": {
        "id": "ZiRPbKY_kWVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We sample multiple sentence(with non-zero `temperature`). Then, we calculate the similarity score(`rougel_similarity`) of all of the sampled sentences with every other. Then we take the average of it to be the score of each sentence. The one with the highest score is selected."
      ],
      "metadata": {
        "id": "lYj1v3B0oOej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(sentence, n_samples, model=None, temperature=0.6):\n",
        "    samples, log_probs = [], []\n",
        "    for _ in range(n_samples):\n",
        "        sample, logp, _ = sampling_decode(sentence, model, temperature)\n",
        "        samples.append(sample)\n",
        "        log_probs.append(logp)\n",
        "    return samples, log_probs"
      ],
      "metadata": {
        "id": "Y1y-HuYlHKek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use ROUGE score as the similarity metric for two sentences.\n",
        "$$ROUGE\\enspace score= 2* \\frac{(precision * recall)}{(precision + recall)}$$"
      ],
      "metadata": {
        "id": "oTdO1XtssZ5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rouge1_similarity(system, reference):\n",
        "    sys_counter = Counter(system)\n",
        "    ref_counter = Counter(reference)\n",
        "    overlap = 0\n",
        "    for token in sys_counter:\n",
        "        token_count_sys = sys_counter[token]\n",
        "        token_count_ref = ref_counter[token]\n",
        "        overlap += min(token_count_ref, token_count_sys)\n",
        "    precision = overlap / sum(sys_counter.values())\n",
        "    recall = overlap / sum(ref_counter.values())\n",
        "    if precision + recall != 0:\n",
        "        rouge1_score = 2 * ((precision * recall)/(precision + recall))\n",
        "    else:\n",
        "        rouge1_score = 0     \n",
        "    return rouge1_score"
      ],
      "metadata": {
        "id": "WBvt4DUpp7QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_avg_overlap(samples, log_probs):\n",
        "    scores = {}\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "        overlap, weight_sum = 0.0, 0.0\n",
        "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):           \n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "            sample_p = float(np.exp(logp))\n",
        "            weight_sum += sample_p\n",
        "            sample_overlap = rouge1_similarity(candidate, sample)\n",
        "            overlap += sample_p * sample_overlap\n",
        "        score = overlap / weight_sum\n",
        "        scores[index_candidate] = score\n",
        "    return scores"
      ],
      "metadata": {
        "id": "esfFWkSEpKry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mbr_decode(sentence, n_samples, model=None, temperature=0.6):\n",
        "    samples, log_probs = generate_samples(sentence, n_samples, model, temperature)\n",
        "    scores = weighted_avg_overlap(samples, log_probs)\n",
        "    max_score_key = max(scores, key=scores.get)\n",
        "    translated_sentence = detokenize(samples[max_score_key], sp_ben_bpe)\n",
        "    return (translated_sentence, max_score_key, scores)"
      ],
      "metadata": {
        "id": "PhD9TykMqenD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I love you\"\n",
        "translated_sentence = mbr_decode(sentence, 4, model, 1.0)\n",
        "print(\"English: \", sentence)\n",
        "print(\"Bengali: \", translated_sentence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6aUMvsqs9jZ",
        "outputId": "7412ad69-2cac-4c6a-879d-c0217cd0958d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:  I love you\n",
            "Bengali:  আমি ঘটবে আমি ভাবছি।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6EoR37lnvRtt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}